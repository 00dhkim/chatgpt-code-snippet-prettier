<!DOCTYPE html>
<html lang="ko">

<head>
    <meta charset="UTF-8">
    <title>ChatGPT Style Extension Before/After Demo</title>
    <style>
        /* 기본 페이지 및 레이아웃 */
        body {
            background-color: #ECECEC;
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            margin: 0;
            padding: 0;
        }

        .demo-wrapper {
            display: flex;
            gap: 20px;
            max-width: 1200px;
            margin: 40px auto;
            padding: 20px;
        }

        .demo-column {
            flex: 1;
            background-color: #FFFFFF;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .demo-column header {
            background-color: #F7F7F8;
            padding: 12px 16px;
            border-bottom: 1px solid #E0E0E0;
            font-size: 16px;
            font-weight: bold;
            color: #333;
        }

        .prompt-container {
            padding: 16px;
            overflow-y: auto;
        }

        /* ChatGPT 대화 메시지 버블 스타일 */
        .chat-bubble {
            background-color: #F8F8F8;
            border-radius: 16px;
            padding: 12px 16px;
            margin-bottom: 16px;
            line-height: 1.5;
            color: #333;
            position: relative;
        }

        .chat-bubble p {
            margin: 0 0 8px 0;
            white-space: pre-wrap;
        }

        /* 기본 코드 블록 스타일 (Before) */
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 12px;
            border-radius: 8px;
            overflow: auto;
            font-family: monospace;
            font-size: 14px;
            margin: 0 0 16px 0;
        }

        /* 확장프로그램 적용 후의 코드 블록 (After) */
        .extension-container pre.user-block {
            display: flex;
            flex-direction: column;
            margin-bottom: -40px;
        }

        .extension-container code.user-block {
            margin-bottom: 40px;
        }

        .extension-container code.collapsed {
            max-height: 200px;
            overflow: hidden;
            margin-bottom: 0;
            position: relative;
        }

        .extension-container code.collapsed::after {
            content: "";
            position: absolute;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 60px;
            background: linear-gradient(to bottom, rgba(45, 45, 45, 0), rgba(45, 45, 45, 1));
            pointer-events: none;
        }

        .extension-container .code-toggle-btn {
            position: relative;
            bottom: 40px;
            left: 50%;
            transform: translateX(-50%);
            padding: 6px 16px;
            font-size: 13px;
            font-weight: 600;
            color: #fff;
            background: linear-gradient(to right, #390f53, #541366);
            border: none;
            border-radius: 6px;
            cursor: pointer;
            box-shadow: 0 3px 8px rgba(0, 0, 0, 0.15);
            transition: all 0.25s ease;
            z-index: 2;
            opacity: 0.95;
        }

        .extension-container .code-toggle-btn:hover {
            background: linear-gradient(to right, #340d44, #43104d);
            transform: translateX(-50%) translateY(-2px);
        }

        .extension-container .code-toggle-btn:active {
            transform: translateX(-50%) translateY(0);
            box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
        }

        /* ChatGPT fade 애니메이션 (선택적) */
        @keyframes _fade_4f9by_7 {
            to {
                opacity: 1;
            }
        }

        ._fadeIn_4f9by_7 {
            animation: _fade_4f9by_7 var(--duration, 0s) cubic-bezier(.37, .55, .86, .88) forwards var(--delay, 0s);
            animation-iteration-count: 1;
            opacity: 0;
        }

        @media (prefers-reduced-motion: reduce) {
            ._fadeIn_4f9by_7 {
                --duration: 0s;
                opacity: 1;
            }
        }
    </style>
</head>

<body>
    <div class="demo-wrapper">
        <!-- Before: 확장프로그램 미적용 -->
        <div id="before-container" class="demo-column">
            <header>Before (No Extension)</header>
            <div class="prompt-container">
                <div class="chat-bubble">
                    <p>compare two codes briefly:</p>
                    <pre>
<code class="language-python">
import numpy as np 
import random 
 
# Define environment size 
grid_size = 4 
goal_state = (3, 3) 
start_state = (0, 0) 
 
# Actions: up, down, left, right 
actions = ['U', 'D', 'L', 'R'] 
action_to_delta = { 
    'U': (-1, 0), 
    'D': (1, 0), 
    'L': (0, -1), 
    'R': (0, 1) 
} 
 
# Q-table initialization 
q_table = {} 
for i in range(grid_size): 
    for j in range(grid_size): 
        q_table[(i, j)] = {a: 0.0 for a in actions} 
 
# Hyperparameters 
alpha = 0.1      # learning rate 
gamma = 0.9      # discount factor 
epsilon = 0.1    # exploration rate 
episodes = 500 
 
# Reward 
def get_reward(state): 
    return 1.0 if state == goal_state else -0.01 
 
# State transition 
def take_action(state, action): 
    di, dj = action_to_delta[action] 
    ni, nj = max(0, min(state[0] + di, grid_size - 1)), max(0, min(state[1] + dj, grid_size - 1)) 
    return (ni, nj) 
 
# Q-learning loop 
for episode in range(episodes): 
    state = start_state 
    while state != goal_state: 
        if random.random() < epsilon: 
            action = random.choice(actions) 
        else: 
            action = max(q_table[state], key=q_table[state].get) 
 
        next_state = take_action(state, action) 
        reward = get_reward(next_state) 
        next_max = max(q_table[next_state].values()) 
         
        # Q-value update 
        q_table[state][action] += alpha * (reward + gamma * next_max - q_table[state][action]) 
        state = next_state 
 
# Show resulting Q-values 
for state in sorted(q_table.keys()): 
    print(f"State {state}: {q_table[state]}")
</code>
          </pre>
                    <pre>
<code class="language-javascript">
const gridSize = 4; 
const actions = ['U', 'D', 'L', 'R']; 
const goalState = [3, 3]; 
const startState = [0, 0]; 
 
// Initialize Q-table 
const qTable = {}; 
for (let i = 0; i < gridSize; i++) { 
    for (let j = 0; j < gridSize; j++) { 
        qTable[`${i},${j}`] = {}; 
        actions.forEach(a => qTable[`${i},${j}`][a] = 0); 
    } 
} 
 
// Parameters 
const alpha = 0.1;    // learning rate 
const gamma = 0.9;    // discount factor 
const epsilon = 0.1;  // exploration rate 
const episodes = 500; 
 
function getReward(state) { 
    return (state[0] === goalState[0] && state[1] === goalState[1]) ? 1.0 : -0.01; 
} 
 
function takeAction(state, action) { 
    let [i, j] = state; 
    switch (action) { 
        case 'U': i = Math.max(0, i - 1); break; 
        case 'D': i = Math.min(gridSize - 1, i + 1); break; 
        case 'L': j = Math.max(0, j - 1); break; 
        case 'R': j = Math.min(gridSize - 1, j + 1); break; 
    } 
    return [i, j]; 
} 
 
function chooseAction(state) { 
    if (Math.random() < epsilon) { 
        return actions[Math.floor(Math.random() * actions.length)]; 
    } else { 
        const qValues = qTable[`${state[0]},${state[1]}`]; 
        return Object.keys(qValues).reduce((a, b) => qValues[a] > qValues[b] ? a : b); 
    } 
} 
 
// Training loop 
for (let ep = 0; ep < episodes; ep++) { 
    let state = [...startState]; 
    while (!(state[0] === goalState[0] && state[1] === goalState[1])) { 
        const action = chooseAction(state); 
        const nextState = takeAction(state, action); 
        const reward = getReward(nextState); 
        const nextQ = Math.max(...Object.values(qTable[`${nextState[0]},${nextState[1]}`])); 
        const currentQ = qTable[`${state[0]},${state[1]}`][action]; 
 
        // Q-learning update 
        qTable[`${state[0]},${state[1]}`][action] = 
            currentQ + alpha * (reward + gamma * nextQ - currentQ); 
 
        state = nextState; 
    } 
} 
 
// Print learned Q-table 
console.log("Learned Q-Table:"); 
Object.entries(qTable).forEach(([state, qValues]) => { 
    console.log(`${state}:`, qValues); 
});
</code>
          </pre>
                </div>
            </div>
        </div>

        <!-- After: 확장프로그램 적용 -->
        <div id="after-container" class="demo-column extension-container">
            <header>After (Extension Applied)</header>
            <div class="prompt-container">
                <div class="chat-bubble">
                    <p>compare two codes briefly:</p>
                    <pre>
<code class="language-python user-block collapsed">
import numpy as np 
import random 
 
# Define environment size 
grid_size = 4 
goal_state = (3, 3) 
start_state = (0, 0) 
 
# Actions: up, down, left, right 
actions = ['U', 'D', 'L', 'R'] 
action_to_delta = { 
    'U': (-1, 0), 
    'D': (1, 0), 
    'L': (0, -1), 
    'R': (0, 1) 
} 
 
# Q-table initialization 
q_table = {} 
for i in range(grid_size): 
    for j in range(grid_size): 
        q_table[(i, j)] = {a: 0.0 for a in actions} 
 
# Hyperparameters 
alpha = 0.1      # learning rate 
gamma = 0.9      # discount factor 
epsilon = 0.1    # exploration rate 
episodes = 500 
 
# Reward 
def get_reward(state): 
    return 1.0 if state == goal_state else -0.01 
 
# State transition 
def take_action(state, action): 
    di, dj = action_to_delta[action] 
    ni, nj = max(0, min(state[0] + di, grid_size - 1)), max(0, min(state[1] + dj, grid_size - 1)) 
    return (ni, nj) 
 
# Q-learning loop 
for episode in range(episodes): 
    state = start_state 
    while state != goal_state: 
        if random.random() < epsilon: 
            action = random.choice(actions) 
        else: 
            action = max(q_table[state], key=q_table[state].get) 
 
        next_state = take_action(state, action) 
        reward = get_reward(next_state) 
        next_max = max(q_table[next_state].values()) 
         
        # Q-value update 
        q_table[state][action] += alpha * (reward + gamma * next_max - q_table[state][action]) 
        state = next_state 
 
# Show resulting Q-values 
for state in sorted(q_table.keys()): 
    print(f"State {state}: {q_table[state]}")
</code>
          </pre>
                    <pre>
<code class="language-javascript user-block collapsed">
const gridSize = 4; 
const actions = ['U', 'D', 'L', 'R']; 
const goalState = [3, 3]; 
const startState = [0, 0]; 
 
// Initialize Q-table 
const qTable = {}; 
for (let i = 0; i < gridSize; i++) { 
    for (let j = 0; j < gridSize; j++) { 
        qTable[`${i},${j}`] = {}; 
        actions.forEach(a => qTable[`${i},${j}`][a] = 0); 
    } 
} 
 
// Parameters 
const alpha = 0.1;    // learning rate 
const gamma = 0.9;    // discount factor 
const epsilon = 0.1;  // exploration rate 
const episodes = 500; 
 
function getReward(state) { 
    return (state[0] === goalState[0] && state[1] === goalState[1]) ? 1.0 : -0.01; 
} 
 
function takeAction(state, action) { 
    let [i, j] = state; 
    switch (action) { 
        case 'U': i = Math.max(0, i - 1); break; 
        case 'D': i = Math.min(gridSize - 1, i + 1); break; 
        case 'L': j = Math.max(0, j - 1); break; 
        case 'R': j = Math.min(gridSize - 1, j + 1); break; 
    } 
    return [i, j]; 
} 
 
function chooseAction(state) { 
    if (Math.random() < epsilon) { 
        return actions[Math.floor(Math.random() * actions.length)]; 
    } else { 
        const qValues = qTable[`${state[0]},${state[1]}`]; 
        return Object.keys(qValues).reduce((a, b) => qValues[a] > qValues[b] ? a : b); 
    } 
} 
 
// Training loop 
for (let ep = 0; ep < episodes; ep++) { 
    let state = [...startState]; 
    while (!(state[0] === goalState[0] && state[1] === goalState[1])) { 
        const action = chooseAction(state); 
        const nextState = takeAction(state, action); 
        const reward = getReward(nextState); 
        const nextQ = Math.max(...Object.values(qTable[`${nextState[0]},${nextState[1]}`])); 
        const currentQ = qTable[`${state[0]},${state[1]}`][action]; 
 
        // Q-learning update 
        qTable[`${state[0]},${state[1]}`][action] = 
            currentQ + alpha * (reward + gamma * nextQ - currentQ); 
 
        state = nextState; 
    } 
} 
 
// Print learned Q-table 
console.log("Learned Q-Table:"); 
Object.entries(qTable).forEach(([state, qValues]) => { 
    console.log(`${state}:`, qValues); 
});
</code>
          </pre>
                    <!-- 토글 버튼은 스크립트에서 동적으로 추가됩니다 -->
                </div>
            </div>
        </div>
    </div>

    <script>
        // 확장프로그램 적용된 영역에서 코드 블록에 토글 버튼 추가
        function styleUserBlock() {
            document.querySelectorAll(".extension-container pre > code.user-block").forEach((codeBlock) => {
                if (codeBlock.parentElement.querySelector("button.code-toggle-btn")) return;
                codeBlock.parentElement.classList.add("user-block");
                codeBlock.classList.add("user-block", "collapsed");

                const toggleButton = document.createElement("button");
                toggleButton.textContent = "Load More";
                toggleButton.className = "code-toggle-btn";

                toggleButton.addEventListener("click", () => {
                    if (codeBlock.classList.contains("collapsed")) {
                        codeBlock.classList.remove("collapsed");
                        toggleButton.textContent = "Load Less";
                    } else {
                        codeBlock.classList.add("collapsed");
                        toggleButton.textContent = "Load More";
                    }
                });

                codeBlock.parentElement.appendChild(toggleButton);
            });
        }

        document.addEventListener("DOMContentLoaded", function () {
            styleUserBlock();
        });
    </script>
</body>

</html>